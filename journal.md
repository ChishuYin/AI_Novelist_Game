# Day 1ï¼šæ¸¸æˆå¯åŠ¨ï¼

## æˆ‘çš„ç›®æ ‡
æˆ‘è¦åšä¸€ä¸ªèƒ½å†™å‡ºã€é‡‘åº¸é£æ ¼ã€‘å°è¯´çš„AIï¼Œå‚è€ƒçš„ä½œå“æœ‰ï¼š

- ã€Šå°„é›•è‹±é›„ä¼ ã€‹ï¼šè‹±é›„æˆé•¿å™äº‹ï¼Œçƒ­è¡€åŠ±å¿—ï¼Œå°‘å¹´æˆä¾ ä¹‹è·¯ï¼›çˆ±å›½æƒ…æ€€æµ“åšï¼ŒèƒŒæ™¯èå…¥å—å®‹æŠ—é‡‘å†å²ï¼›æ­£é‚ªåˆ†æ˜ï¼Œäººç‰©å½¢è±¡é²œæ˜ï¼ˆéƒ­é–çš„å¿ åšã€é»„è“‰çš„èªæ…§ï¼‰ï¼›æƒ…èŠ‚ç´§å‡‘ï¼Œå…¼å…·å®¶ä»‡å›½æ¨ä¸å„¿å¥³æƒ…é•¿ï¼›æ–‡é£æ˜å¿«å¤§æ°”ï¼Œå……æ»¡ç†æƒ³ä¸»ä¹‰
- ã€Šå¤©é¾™å…«éƒ¨ã€‹ï¼šç»“æ„å®å¤§ï¼Œå¤šçº¿äº¤ç»‡ï¼Œäººç‰©ä¼—å¤šä¸”æ€§æ ¼å¤æ‚ï¼›æ¢è®¨â€œäººæ€§æœ¬æ¶â€ã€â€œä½›æ€§ä¸é­”æ€§â€çš„å“²å­¦å‘½é¢˜ï¼›æƒ…èŠ‚èµ·ä¼è·Œå®•ï¼Œå‘½è¿æ²‰é‡ï¼Œå¸¦æœ‰å¼ºçƒˆå®¿å‘½æ„Ÿ
- ã€Šç¬‘å‚²æ±Ÿæ¹–ã€‹ï¼šè®½å–»ç°å®ï¼Œä¸»é¢˜æ·±åˆ»ï¼ˆæƒåŠ›æ–—äº‰ã€è‡ªç”± vs. è§„è®­ï¼‰ï¼›ä¸»è§’å­¤å‚²æ´’è„±ï¼Œåä½“åˆ¶ï¼Œæƒ…æ„Ÿå¤æ‚ï¼›æ–‡é£å…¼å…·å¹½é»˜ä¸å“²ç†ï¼Œè¯­è¨€åç¾è€Œå¯Œæœ‰æ–‡é‡‡
- ã€Šé¹¿é¼è®°ã€‹ï¼šå®Œå…¨åç±»å‹åŒ–ï¼Œä¸»è§’â€œéä¾ â€å´çºµæ¨ªå¤©ä¸‹ï¼›è®½åˆºæƒåŠ›ã€è’è¯å¹½é»˜ï¼Œé¢ è¦†ä¼ ç»Ÿæ­¦ä¾ ç»“æ„ï¼›è¯­è¨€è¯™è°ã€å¯¹ç™½æœºæ™ºï¼Œç°å®ä¸»ä¹‰è‰²å½©æµ“åš

## ä»Šæ—¥è¿›åº¦
å®Œæˆäº†å¼€å‘ç¯å¢ƒæ­å»ºï¼Œæ³¨å†Œäº†HuggingFaceè´¦å·ã€‚

## æƒ³è±¡ä¸­çš„AI
æˆ‘å¸Œæœ›å®ƒèƒ½æ¥è¿‡æˆ‘å†™çš„å°è¯´å¤§çº²ï¼Œè‡ªåŠ¨å¸®æˆ‘å†™å‡ºç²¾å½©çš„æ®µè½ã€‚å°±åƒæˆ‘è„‘æµ·é‡Œçš„â€œæ›¿èº«å†™æ‰‹â€ã€‚

## ä»Šæ—¥å¥–åŠ±
ç§°å·ï¼šé€ æ¢¦ä¹‹åˆ
æŠ€èƒ½ç‚¹ +1


# Day 1: Game Start!

## My Goal  
I want to build an AI that can write novels in the **style of Jin Yong**. The reference works include:

- *The Legend of the Condor Heroes*: A classic heroâ€™s journey with passionate and inspiring growth; strong patriotic themes set in the context of the Southern Song Dynasty's resistance against the Jin invaders; clear distinction between good and evil, with vivid characters (e.g., Guo Jingâ€™s honesty and Huang Rongâ€™s cleverness); tightly woven plot combining national crisis and personal emotions; energetic and idealistic writing style.

- *Demi-Gods and Semi-Devils*: Grand structure with interwoven storylines and a vast cast of complex characters; explores philosophical themes like the duality of human nature and the tension between Buddhist compassion and inner darkness; dramatic ups and downs with a heavy sense of fate.

- *The Smiling, Proud Wanderer*: A sharp satire on reality with deep themes (power struggles, freedom vs. constraint); a free-spirited and emotionally layered protagonist who resists the system; combines humor and philosophy with elegant, poetic prose.

- *The Deer and the Cauldron*: A complete genre subversionâ€”an anti-hero protagonist navigating the world with wit; sharp satire on power, absurd humor, and a break from traditional wuxia tropes; witty dialogues, humorous narration, and strong realism.

## Progress Today  
Completed development environment setup and registered a HuggingFace account.

## My Dream AI  
I hope it can take the outlines I write and turn them into vivid, captivating passagesâ€”like a â€œghostwriter in my mind.â€

## Todayâ€™s Reward  
**Title**: *The Dream Begins*  
**Skill Point**: +1










# Day 2ï¼šè¯­æ–™çŒäºº

## æˆ‘è·å–äº†è¿™äº›å°è¯´ï¼š
- ã€Šå°„é›•è‹±é›„ä¼ ã€‹ï¼šç½‘ç›˜ä¸‹è½½
- ã€Šå€šå¤©å± é¾™è®°ã€‹ï¼šç½‘ç›˜ä¸‹è½½
- ã€Šé¹¿é¼è®°ã€‹ï¼šç½‘ç›˜ä¸‹è½½
- ã€Šç¬‘å‚²æ±Ÿæ¹–ã€‹ï¼šç½‘ç›˜ä¸‹è½½
- ã€Šå¤©é¾™å…«éƒ¨ã€‹ï¼šç½‘ç›˜ä¸‹è½½

## æ¸…æ´—æ–¹å¼
- æ¯éƒ¨å°è¯´éƒ½åˆ†æˆä¸Šä¸‹ä¸¤éƒ¨ï¼ˆå°½é‡æ§åˆ¶åœ¨2mbä»¥ä¸‹ï¼‰
- å»æ‰äº†å¥å­ä¸­é—´çš„æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼

## æ˜æ—¥è®¡åˆ’
æ„å»ºç»“æ„åŒ–æ•°æ®é›†ï¼Œç”¨ Dataset åŠ è½½è¿™äº›è¯­æ–™ï¼

## ä»Šæ—¥ç§°å·
ğŸ“– æ–‡æœ¬é‡‡é›†è€…

## ä»Šæ—¥å¥–åŠ±
æŠ€èƒ½ç‚¹ +1


# Day 2: Corpus Hunter

## ğŸ“š Novels I Collected
- *The Legend of the Condor Heroes* â€“ cloud download  
- *The Heaven Sword and Dragon Saber* â€“ cloud download  
- *The Deer and the Cauldron* â€“ cloud download  
- *The Smiling, Proud Wanderer* â€“ cloud download  
- *Demi-Gods and Semi-Devils* â€“ cloud download  

## ğŸ§¼ Cleaning Methods
- Each novel was split into **two parts**, keeping each file **under ~2MB**
- Removed **line breaks inside sentences** and cleaned **extra whitespace**

## ğŸ”® Tomorrowâ€™s Plan
Build a **structured dataset** and load the corpora using `datasets.Dataset` from HuggingFace.

## ğŸ·ï¸ Title of the Day
**ğŸ“– The Text Collector**

## ğŸ Todayâ€™s Reward
**+1 Skill Point**










# Day 3ï¼šç»“æ„ç¥­å›

## ä»Šæ—¥è¿›å±•
- æˆåŠŸå°†å°è¯´æ–‡æœ¬åˆ†æ®µï¼Œæ„å»ºå‡ºç»“æ„åŒ– Dataset
- æ•°æ®é‡ï¼šçº¦ xxx ä¸ªæ®µè½
- æ¯æ®µå¹³å‡å­—æ•°ï¼šxxx

## æŠ€æœ¯ç»†èŠ‚
- ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°å°†æ–‡æœ¬æŒ‰é€»è¾‘å¥å­åˆ‡å—
- ä½¿ç”¨ HuggingFace Dataset ä¿å­˜ä¸ºæœ¬åœ°ç£ç›˜æ ¼å¼

## æ˜æ—¥ç›®æ ‡
å¼€å§‹å‡†å¤‡å¾®è°ƒæ¨¡å‹ï¼Œå®šä¹‰è®­ç»ƒå‚æ•°

## ä»Šæ—¥ç§°å·
ğŸ§ª æ•°æ®ç‚¼é‡‘æœ¯å£«

## ä»Šæ—¥å¥–åŠ±
æŠ€èƒ½ç‚¹ +1


# Day 3: The Structure Altar

## Progress Today
- Successfully segmented the novel text into paragraphs and constructed a structured Dataset
- Data volume: approximately xxx paragraphs  
- Average characters per paragraph: xxx

## Technical Details
- Used a custom function to split the text into logical sentence chunks  
- Saved using HuggingFace `Dataset` in local disk format

## Goals for Tomorrow
Begin preparing for model fine-tuning and define training parameters

## Title of the Day
ğŸ§ª Data Alchemist

## Daily Reward
+1 Skill Point










# ğŸ“… Day 4ï¼šæ¨¡å‹é©¯å…»ä¹‹é—¨ï¼ˆè½»é‡ç‰ˆï¼‰

## ğŸ¯ ä»Šæ—¥ä»»åŠ¡
ä¸ºä¸­æ–‡ GPT2 å°æ¨¡å‹å¾®è°ƒåšå¥½å‡†å¤‡å·¥ä½œï¼Œæ„å»º tokenizerã€æ•°æ®å¤„ç†å‡½æ•°ä¸è®­ç»ƒè„šæœ¬æ¡†æ¶ã€‚

## âœ… ä»Šæ—¥å®Œæˆäº‹é¡¹
- æˆåŠŸé€‰æ‹©å¹¶åŠ è½½äº† HuggingFace ä¸­æ–‡ GPT2 å°æ¨¡å‹
- ä½¿ç”¨ AutoTokenizer æˆåŠŸè§£ææ•°æ®ï¼ˆmax_length=512ï¼‰
- æ­å»ºäº†æœ¬åœ° LoRA è®­ç»ƒè„šæœ¬ï¼štrain_gpt2_chinese_lora.py
- ä½¿ç”¨ PEFT å’Œ LoRA å‚æ•°é…ç½®å®Œæˆè®­ç»ƒæ¡†æ¶
- å°† tokenizer ä¿å­˜åˆ°æœ¬åœ°ç›®å½•ï¼š`tokenizer_gpt2_chinese/`

## ğŸ”§ æŠ€æœ¯è¦ç‚¹
- å­¦ä¼šä½¿ç”¨ tokenizer ä¸ dataset ç»“åˆè¿›è¡Œæ‰¹é‡ tokenize
- ç†Ÿæ‚‰è®­ç»ƒè„šæœ¬æ‰€éœ€ç»“æ„ï¼ˆTrainer + Dataset + LoRAï¼‰

## ğŸ ä»Šæ—¥ç§°å·
ğŸ æ–‡é£ç»ƒä¹ ç”Ÿ

## ğŸ§  æŠ€èƒ½ç‚¹ +2
- æŒæ¡ tokenizer ä¸æ¨¡å‹çš„åŸºæœ¬åä½œæµç¨‹
- ç†Ÿæ‚‰ LoRA å¾®è°ƒæµç¨‹ä¸æ¨¡å—ç»“æ„


# ğŸ“… Day 4: Model Taming Begins (Lightweight Version)

## ğŸ¯ Goal
Prepare for fine-tuning a small Chinese GPT2 model with PEFT/LoRA, including tokenizer setup and training script construction.

## âœ… Tasks Completed
- Selected pretrained model: `uer/gpt2-chinese-cluecorpussmall`
- Loaded tokenizer using `AutoTokenizer`, configured for 512 tokens
- Constructed LoRA training script `train_gpt2_chinese_lora.py`
- Configured LoRA parameters and PEFT integration
- Saved tokenizer locally to `tokenizer_gpt2_chinese/`

## ğŸ”§ Technical Highlights
- Learned to batch tokenize with HuggingFace tokenizer + datasets
- Built a modular training pipeline using Trainer + PEFT + LoRA

## ğŸ Title Earned
ğŸ Stylist Apprentice

## ğŸ§  +2 Skill Points
- Mastered tokenizer/model interaction
- Learned the LoRA micro-finetuning setup











# ğŸ“… Day 5ï¼šå°è¯´é£æ ¼å¾®è°ƒè®­ç»ƒï¼ˆæœ¬åœ°ï¼‰

## ğŸ¯ ä»Šæ—¥ä»»åŠ¡
ä½¿ç”¨ uer/gpt2-chinese-cluecorpussmall æ¨¡å‹ï¼Œåœ¨æœ¬åœ° GPU ä¸Šè¿›è¡Œå°è¯´æ–‡æœ¬çš„å¾®è°ƒè®­ç»ƒï¼Œé‡‡ç”¨ LoRA æŠ€æœ¯è¿›è¡Œè½»é‡å‚æ•°è°ƒä¼˜ã€‚

## âœ… ä»Šæ—¥å®Œæˆäº‹é¡¹
- æˆåŠŸåŠ è½½ HuggingFace ä¸­æ–‡ GPT2 å°æ¨¡å‹
- ä½¿ç”¨ HuggingFace Dataset åŠ è½½æ ¼å¼åŒ–å°è¯´æ•°æ®
- é…ç½® LoRA å‚æ•°ï¼ˆr=8, alpha=16, target_modules=["c_attn"]ï¼‰
- åœ¨ GTX 1660 Super ä¸ŠæˆåŠŸè®­ç»ƒ 3 ä¸ª epoch
- æ¨¡å‹ä¿å­˜è·¯å¾„ï¼š`outputs/gpt2_chinese_lora_small/`
- è¾“å‡ºæ–‡ä»¶åŒ…æ‹¬ `adapter_model.safetensors` ä¸ `adapter_config.json`

## ğŸ”§ æŠ€æœ¯è¦ç‚¹
- ä½¿ç”¨ PEFT + LoRA è®­ç»ƒå¤§å¤§é™ä½äº†æ˜¾å­˜éœ€æ±‚
- è®­ç»ƒæ— éœ€ä½¿ç”¨ bitsandbytes æˆ– GPU é‡åŒ–
- è¾“å‡ºæ¨¡å‹éœ€è¦ä¸ base_model ä¸€èµ·åŠ è½½æ‰èƒ½ä½¿ç”¨

## ğŸ ä»Šæ—¥ç§°å·
ğŸ§™â€â™€ï¸ æ–‡é£å¡‘å½¢è€…

## ğŸ§  æŠ€èƒ½ç‚¹ +2
- ç†Ÿç»ƒæŒæ¡ tokenizer + æ•°æ®åˆ‡åˆ† + LoRA è®­ç»ƒæµç¨‹
- æŒæ¡ HuggingFace å¾®è°ƒç»“æ„ä¸ä¿å­˜æ–¹å¼

## ğŸ“Œ ä¸‹ä¸€æ­¥è®¡åˆ’
- ç¼–å†™æ¨ç†æµ‹è¯•è„šæœ¬ï¼Œè¾“å…¥å°è¯´æç¤ºè¯­å¹¶ç”Ÿæˆé£æ ¼åŒ–ç»­å†™
- å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œä¸»è§‚é£æ ¼è¯„åˆ†

# ğŸ“… Day 5: Fine-tuning for Novel Style (Local Training)

## ğŸ¯ Todayâ€™s Goal
Fine-tune the pretrained model `uer/gpt2-chinese-cluecorpussmall` on cleaned novel datasets using LoRA on local GPU (GTX 1660 Super).

## âœ… Tasks Completed
- Loaded pretrained GPT2 Chinese small model via HuggingFace
- Loaded preprocessed dataset from disk using `datasets`
- Configured LoRA (r=8, alpha=16, target_modules=["c_attn"])
- Trained for 3 epochs successfully on local GPU
- Model saved to `outputs/gpt2_chinese_lora_small/`
- Output includes `adapter_model.safetensors` and `adapter_config.json`

## ğŸ”§ Key Technical Notes
- LoRA enables low-resource fine-tuning
- No bitsandbytes or GPU quantization needed
- Inference requires loading base model + adapter

## ğŸ New Title
ğŸ§™â€â™€ï¸ Stylist of Sentences

## ğŸ§  +2 Skill Points
- Mastered tokenizer usage, dataset preprocessing, LoRA training
- Learned HuggingFace + PEFT save/load pipeline

## ğŸ“Œ Next Step
- Write an inference script to test style continuation
- Rate model output for stylistic accuracy










# ğŸ“… Day 6ï¼šé£æ ¼è§‰é†’æµ‹è¯•

## ğŸ¯ ä»Šæ—¥ä»»åŠ¡
ä½¿ç”¨å¾®è°ƒåçš„ GPT2 ä¸­æ–‡å°æ¨¡å‹ç”Ÿæˆå°è¯´æ®µè½ï¼Œè¿›è¡Œä¸»è§‚é£æ ¼è¯„ä¼°ï¼Œè¯†åˆ«æ¨¡å‹èƒ½åŠ›ä¸å±€é™ã€‚

## âœ… ä»»åŠ¡å®Œæˆ
- æˆåŠŸè¿è¡Œæ¨ç†è„šæœ¬ `infer_gpt2_chinese_lora.py`
- ä½¿ç”¨ Promptï¼šâ€œå¥¹ç«™åœ¨æ¡¥å¤´ï¼Œæœ›ç€é›¨é›¾ä¸­çš„åŸå¢™ï¼Œå¿ƒé‡Œå¿½ç„¶æ³›èµ·ä¸€ç§å¥‡æ€ªçš„æƒ…ç»ªã€‚â€
- æ¨¡å‹æˆåŠŸç”Ÿæˆ 2~3 æ®µè½ï¼Œè¯­æ„Ÿåˆå…·å¤é£ï¼Œä½†å­˜åœ¨é‡å¤ä¸è·³è·ƒé—®é¢˜

## ğŸ” é£æ ¼è¯„åˆ†ï¼ˆä¸»è§‚ï¼‰
- ç”¨è¯è‡ªç„¶åº¦ï¼š3.5/5
- å¥æ³•é€šé¡ºï¼š4/5
- æƒ…èŠ‚è¿è´¯æ€§ï¼š2/5
- é£æ ¼æ¨¡ä»¿æ€§ï¼š3.5/5

## ğŸ§  è®¤çŸ¥æå‡
å°æ¨¡å‹åœ¨é£æ ¼ä»¿å†™æ–¹é¢å·²æœ‰æ‰€æˆï¼Œä½†å¥å­å±‚é¢æ¼‚ç§»æ˜æ˜¾ï¼Œéš¾ä»¥ç”Ÿæˆç¨³å®šçš„å‰§æƒ…æ®µè½ â†’ å‡†å¤‡å‡çº§åˆ°å¤§æ¨¡å‹ï¼ˆDeepSeekï¼‰

## ğŸ ä»Šæ—¥ç§°å·
ğŸ“ é£æ ¼è§‚å¯Ÿè€…

## ğŸ§  æŠ€èƒ½ç‚¹ +1
- æŒæ¡å¾®è°ƒæ¨¡å‹è¯„ä¼°æ–¹æ³•
- åˆæ­¥äº†è§£ prompt â†’ è¾“å‡ºæ•ˆæœçš„å½±å“å› ç´ 


# ğŸ“… Day 6: Style Awakening Test

## ğŸ¯ Today's Task
Use the fine-tuned GPT2 Chinese small model to generate novel passages, and evaluate the stylistic quality of the outputs.

## âœ… Completed Tasks
- Ran the inference script `infer_gpt2_chinese_lora.py`
- Prompt used: â€œShe stood on the bridge, staring at the fog-covered city wallâ€¦â€
- Output was grammatically correct and stylistically similar to training data, but suffered from repetition and incoherence.

## ğŸ” Subjective Style Evaluation
- Word Choice Naturalness: 3.5/5  
- Sentence Flow: 4/5  
- Narrative Coherence: 2/5  
- Style Mimicry: 3.5/5  

## ğŸ§  Key Insight
Small models can mimic style, but struggle with continuity and structural control. Prepare to upgrade to DeepSeek for larger-scale generation.

## ğŸ Title Unlocked
ğŸ“ Style Observer

## ğŸ§  +1 Skill Point
- Learned how to evaluate fine-tuned model behavior
- Understood the impact of prompt structure on model output
