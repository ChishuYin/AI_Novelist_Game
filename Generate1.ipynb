{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa396ec0-8a0f-4f2e-9379-514dfc1813ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer_gpt2_chinese_lora.py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d7ee3f4-80e7-4b38-98d3-ee47d28d91ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(21128, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Conv1D(nf=768, nx=768)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=3072)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ¨¡å‹è·¯å¾„\n",
    "base_model = \"uer/gpt2-chinese-cluecorpussmall\"\n",
    "lora_model_path = \"outputs/gpt2_chinese_lora_small\"\n",
    "\n",
    "# åŠ è½½ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"outputs/gpt2_chinese_lora_small\", use_fast=False) #use_fastå’Œä¸­æ–‡å…¼å®¹ä¸å¥½\n",
    "\n",
    "# åŠ è½½åŸºç¡€æ¨¡å‹ + LoRA adapter\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "model = PeftModel.from_pretrained(model, lora_model_path)\n",
    "model.eval() #è¿›å…¥æ¨ç†æ¨¡å¼ï¼Œå…³é—­ dropout å’Œæ¢¯åº¦è®¡ç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cf7f398-cf5d-4f7e-99f3-3b10443c8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆå‡½æ•°\n",
    "def generate(text, max_tokens=50):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device) #ç”¨ tokenizer å°†è¾“å…¥çš„åŸå§‹æ–‡æœ¬è½¬æˆ tokenï¼ˆtensor æ ¼å¼ï¼‰ï¼Œä¾‹å¦‚ [[101, 2345, 102]]ï¼›return_tensors=\"pt\" è¡¨ç¤ºè¿”å› PyTorch æ ¼å¼ï¼›.to(model.device) æŠŠè¾“å…¥é€åˆ°å’Œæ¨¡å‹ä¸€æ ·çš„è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰ï¼Œé˜²æ­¢â€œè®¾å¤‡ä¸ä¸€è‡´â€çš„é”™è¯¯ã€‚\n",
    "    with torch.no_grad(): #å…³é—­æ¢¯åº¦è®¡ç®—ï¼ŒåŠ é€Ÿæ¨ç†ï¼›\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True, #ä½¿ç”¨éšæœºé‡‡æ ·è€Œéè´ªå©ªè§£ç ï¼›\n",
    "            top_p=0.99, #æ ¸é‡‡æ ·ï¼ˆåªä¿ç•™å‰ 95% æ¦‚ç‡çš„ tokenï¼‰ï¼›\n",
    "            temperature=0.95 #æ¸©åº¦ç³»æ•°ï¼ˆå€¼è¶Šå°è¶Šä¿å®ˆï¼Œè¶Šå¤§è¶Šå‘æ•£ï¼‰ï¼›\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True) #å°†è¾“å‡ºçš„ token id è½¬æ¢æˆå¯è¯»æ–‡æœ¬ï¼›skip_special_tokens=Trueï¼šè·³è¿‡ <pad>, <bos> ç­‰ç‰¹æ®Šç¬¦å·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f7667d1-2bba-4032-a069-f3e033035858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“œ ç”Ÿæˆå†…å®¹ï¼š\n",
      " å¥¹ ç«™ åœ¨ æ¡¥ å¤´ ï¼Œ æœ› ç€ é›¨ é›¾ ä¸­ çš„ åŸ å¢™ ï¼Œ å¿ƒ é‡Œ å¿½ ç„¶ æ³› èµ· ä¸€ ç§ å¥‡ æ€ª çš„ æƒ… ç»ª ã€‚ ä¸Š å®˜ å©‰ å„¿ æ˜¯ å¦‚ ä½• å› è½¬ ç€ èµ° æ¥ çš„ ï¼Ÿ æ˜¯ è° å·² ç» èµ° äº† å¤š å°‘ æ­¥ ï¼Ÿ ä»– åœ¨ æ¡¥ ä¸Š åˆ æ˜¯ èµ° äº† å¤š å°‘ æ­¥ ï¼Ÿ é‚£ äº› äºº éƒ½ ç«™ åœ¨ å“ª é‡Œ ï¼Ÿ ä»– ä¸ å† æƒ³ èµ°\n"
     ]
    }
   ],
   "source": [
    "# ç¤ºä¾‹è°ƒç”¨\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"å¥¹ç«™åœ¨æ¡¥å¤´ï¼Œæœ›ç€é›¨é›¾ä¸­çš„åŸå¢™ï¼Œå¿ƒé‡Œå¿½ç„¶æ³›èµ·ä¸€ç§å¥‡æ€ªçš„æƒ…ç»ªã€‚\"\n",
    "    output = generate(prompt)\n",
    "    print(\"ğŸ“œ ç”Ÿæˆå†…å®¹ï¼š\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a189935-9977-40b5-ac8a-9a06eadfbad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
