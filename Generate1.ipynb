{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa396ec0-8a0f-4f2e-9379-514dfc1813ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer_gpt2_chinese_lora.py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d7ee3f4-80e7-4b38-98d3-ee47d28d91ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(21128, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Conv1D(nf=768, nx=768)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=3072)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型路径\n",
    "base_model = \"uer/gpt2-chinese-cluecorpussmall\"\n",
    "lora_model_path = \"outputs/gpt2_chinese_lora_small\"\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"outputs/gpt2_chinese_lora_small\", use_fast=False) #use_fast和中文兼容不好\n",
    "\n",
    "# 加载基础模型 + LoRA adapter\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "model = PeftModel.from_pretrained(model, lora_model_path)\n",
    "model.eval() #进入推理模式，关闭 dropout 和梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cf7f398-cf5d-4f7e-99f3-3b10443c8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成函数\n",
    "def generate(text, max_tokens=50):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device) #用 tokenizer 将输入的原始文本转成 token（tensor 格式），例如 [[101, 2345, 102]]；return_tensors=\"pt\" 表示返回 PyTorch 格式；.to(model.device) 把输入送到和模型一样的设备（GPU 或 CPU），防止“设备不一致”的错误。\n",
    "    with torch.no_grad(): #关闭梯度计算，加速推理；\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True, #使用随机采样而非贪婪解码；\n",
    "            top_p=0.99, #核采样（只保留前 95% 概率的 token）；\n",
    "            temperature=0.95 #温度系数（值越小越保守，越大越发散）；\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True) #将输出的 token id 转换成可读文本；skip_special_tokens=True：跳过 <pad>, <bos> 等特殊符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f7667d1-2bba-4032-a069-f3e033035858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📜 生成内容：\n",
      " 她 站 在 桥 头 ， 望 着 雨 雾 中 的 城 墙 ， 心 里 忽 然 泛 起 一 种 奇 怪 的 情 绪 。 上 官 婉 儿 是 如 何 回 转 着 走 来 的 ？ 是 谁 已 经 走 了 多 少 步 ？ 他 在 桥 上 又 是 走 了 多 少 步 ？ 那 些 人 都 站 在 哪 里 ？ 他 不 再 想 走\n"
     ]
    }
   ],
   "source": [
    "# 示例调用\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"她站在桥头，望着雨雾中的城墙，心里忽然泛起一种奇怪的情绪。\"\n",
    "    output = generate(prompt)\n",
    "    print(\"📜 生成内容：\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a189935-9977-40b5-ac8a-9a06eadfbad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
